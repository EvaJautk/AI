{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unt93_KRgW3Q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import sobel\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Load the data\n",
        "def load_data():\n",
        "    path_to_train_data = 'flatland_train.data'\n",
        "    path_to_test_data = 'flatland_test.data'\n",
        "\n",
        "    # Load training data\n",
        "    with gzip.open(path_to_train_data, 'rb') as f:\n",
        "        X_train, y_train = pickle.load(f)\n",
        "\n",
        "    # Load testing data\n",
        "    with gzip.open(path_to_test_data, 'rb') as f:\n",
        "        X_test, y_test = pickle.load(f)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "X, y, X_p, y_p = load_data()\n",
        "\n",
        "# Normalize and convert the data to tensors\n",
        "X = tf.convert_to_tensor(X, dtype=tf.float32) / 255.0\n",
        "X_p = tf.convert_to_tensor(X_p, dtype=tf.float32) / 255.0\n",
        "\n",
        "# Function to apply Sobel edge detection with padding\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def apply_sobel_edges_with_padding(images, pad_width):\n",
        "    # Pad images\n",
        "    padded_images = tf.pad(images, [[0, 0], [pad_width, pad_width], [pad_width, pad_width]], mode='CONSTANT')\n",
        "\n",
        "    # Define Sobel kernels\n",
        "    sobel_x = tf.constant([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype=tf.float32)\n",
        "    sobel_y = tf.constant([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=tf.float32)\n",
        "\n",
        "    # Reshape kernels to match the expected dimensions for Conv2D\n",
        "    sobel_x = tf.expand_dims(tf.expand_dims(sobel_x, axis=-1), axis=-1)  # Shape: (3, 3, 1, 1)\n",
        "    sobel_y = tf.expand_dims(tf.expand_dims(sobel_y, axis=-1), axis=-1)  # Shape: (3, 3, 1, 1)\n",
        "\n",
        "    # Apply Sobel filters using tf.nn.conv2d\n",
        "    gradient_x = tf.nn.conv2d(padded_images[..., tf.newaxis], sobel_x, strides=[1, 1, 1, 1], padding='VALID')\n",
        "    gradient_y = tf.nn.conv2d(padded_images[..., tf.newaxis], sobel_y, strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "    # Compute the gradient magnitude\n",
        "    gradient_magnitude = tf.sqrt(tf.square(gradient_x) + tf.square(gradient_y))\n",
        "\n",
        "    return tf.squeeze(gradient_magnitude, axis=-1)\n",
        "\n",
        "# Example usage\n",
        "pad_width = 1\n",
        "X_edges = apply_sobel_edges_with_padding(X, pad_width)\n",
        "X_p_edges = apply_sobel_edges_with_padding(X_p, pad_width)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply Sobel edges and add channel dimension\n",
        "pad_width = 1\n",
        "X_edges = apply_sobel_edges_with_padding(X, pad_width)\n",
        "X_p_edges = apply_sobel_edges_with_padding(X_p, pad_width)\n",
        "X_edges = np.expand_dims(X_edges, axis=-1)\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "train_datagen.fit(X_edges)\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_edges, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Input(shape=(52, 52, 1)))\n",
        "\n",
        "# Define filter sizes and dropout rates for each layer\n",
        "layer_configs = [\n",
        "    {\"filters\": 16, \"dropout\": 0.2},\n",
        "    {\"filters\": 32, \"dropout\": 0.3},\n",
        "    {\"filters\": 64, \"dropout\": 0.4},\n",
        "    {\"filters\": 128, \"dropout\": 0.5}\n",
        "]\n",
        "\n",
        "# Add convolutional layers in a loop\n",
        "for config in layer_configs:\n",
        "    model.add(layers.Conv2D(config[\"filters\"], (3, 3), activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(config[\"dropout\"]))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Custom Callback for Printing Epoch Info\n",
        "class CustomEpochLogger(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        accuracy = logs.get('accuracy')\n",
        "        loss = logs.get('loss')\n",
        "        print(f'Epoch {epoch + 1}: accuracy: {accuracy:.4f}, loss: {loss:.4f}')\n",
        "\n",
        "# Model checkpointing\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='model_epoch_{epoch:02d}.weights.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=0  # Suppress model saving output\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "train_generator = train_datagen.flow(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=55,  # Increased epochs for better training\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[checkpoint_callback, CustomEpochLogger()],\n",
        "    verbose=0  # Suppress default logging\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'\\nTest accuracy: {test_accuracy:.6f}')\n",
        "\n",
        "# Predict on X_p_edges and display predictions\n",
        "X_p_edges = np.expand_dims(X_p_edges, axis=-1)\n",
        "predictions = model.predict(X_p_edges)\n",
        "predicted_labels = [np.argmax(pred) for pred in predictions]\n",
        "formatted_predictions = ''.join([str(round(p)) for p in predicted_labels])\n",
        "print(\"Predicted labels:\", formatted_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLJVoxcNVInl",
        "outputId": "fcf0a2a1-2634-4c67-924f-354365f95f63"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: accuracy: 0.2635, loss: 2.0659\n",
            "Epoch 2: accuracy: 0.3250, loss: 1.5566\n",
            "Epoch 3: accuracy: 0.4009, loss: 1.3880\n",
            "Epoch 4: accuracy: 0.4403, loss: 1.2831\n",
            "Epoch 5: accuracy: 0.4920, loss: 1.1754\n",
            "Epoch 6: accuracy: 0.5169, loss: 1.0979\n",
            "Epoch 7: accuracy: 0.5641, loss: 1.0227\n",
            "Epoch 8: accuracy: 0.6010, loss: 0.9600\n",
            "Epoch 9: accuracy: 0.6258, loss: 0.8918\n",
            "Epoch 10: accuracy: 0.6593, loss: 0.8545\n",
            "Epoch 11: accuracy: 0.6858, loss: 0.7891\n",
            "Epoch 12: accuracy: 0.7034, loss: 0.7672\n",
            "Epoch 13: accuracy: 0.7221, loss: 0.7128\n",
            "Epoch 14: accuracy: 0.7465, loss: 0.6880\n",
            "Epoch 15: accuracy: 0.7684, loss: 0.6426\n",
            "Epoch 16: accuracy: 0.7841, loss: 0.6110\n",
            "Epoch 17: accuracy: 0.7977, loss: 0.5819\n",
            "Epoch 18: accuracy: 0.8062, loss: 0.5572\n",
            "Epoch 19: accuracy: 0.8217, loss: 0.5229\n",
            "Epoch 20: accuracy: 0.8223, loss: 0.5125\n",
            "Epoch 21: accuracy: 0.8349, loss: 0.4944\n",
            "Epoch 22: accuracy: 0.8388, loss: 0.4872\n",
            "Epoch 23: accuracy: 0.8519, loss: 0.4628\n",
            "Epoch 24: accuracy: 0.8506, loss: 0.4550\n",
            "Epoch 25: accuracy: 0.8601, loss: 0.4354\n",
            "Epoch 26: accuracy: 0.8656, loss: 0.4370\n",
            "Epoch 27: accuracy: 0.8702, loss: 0.4270\n",
            "Epoch 28: accuracy: 0.8758, loss: 0.4044\n",
            "Epoch 29: accuracy: 0.8795, loss: 0.3807\n",
            "Epoch 30: accuracy: 0.8865, loss: 0.3795\n",
            "Epoch 31: accuracy: 0.8830, loss: 0.3812\n",
            "Epoch 32: accuracy: 0.8936, loss: 0.3617\n",
            "Epoch 33: accuracy: 0.8935, loss: 0.3569\n",
            "Epoch 34: accuracy: 0.8930, loss: 0.3591\n",
            "Epoch 35: accuracy: 0.8996, loss: 0.3519\n",
            "Epoch 36: accuracy: 0.8997, loss: 0.3524\n",
            "Epoch 37: accuracy: 0.9016, loss: 0.3406\n",
            "Epoch 38: accuracy: 0.9028, loss: 0.3418\n",
            "Epoch 39: accuracy: 0.9031, loss: 0.3440\n",
            "Epoch 40: accuracy: 0.9060, loss: 0.3184\n",
            "Epoch 41: accuracy: 0.9079, loss: 0.3293\n",
            "Epoch 42: accuracy: 0.9096, loss: 0.3212\n",
            "Epoch 43: accuracy: 0.9144, loss: 0.3154\n",
            "Epoch 44: accuracy: 0.9090, loss: 0.3172\n",
            "Epoch 45: accuracy: 0.9136, loss: 0.3027\n",
            "Epoch 46: accuracy: 0.9161, loss: 0.3147\n",
            "Epoch 47: accuracy: 0.9168, loss: 0.3016\n",
            "Epoch 48: accuracy: 0.9206, loss: 0.3060\n",
            "Epoch 49: accuracy: 0.9169, loss: 0.3048\n",
            "Epoch 50: accuracy: 0.9191, loss: 0.3077\n",
            "Epoch 51: accuracy: 0.9220, loss: 0.2952\n",
            "Epoch 52: accuracy: 0.9233, loss: 0.2882\n",
            "Epoch 53: accuracy: 0.9197, loss: 0.2961\n",
            "Epoch 54: accuracy: 0.9246, loss: 0.2844\n",
            "Epoch 55: accuracy: 0.9236, loss: 0.2919\n",
            "\n",
            "Test accuracy: 0.963500\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step\n",
            "Predicted labels: 6646533043400063340545465645640463060355660333004303364366664004553303655503043350530664000355365463530403365345345040035360650004655530445455543654345046545635035636546003536656366660360005666463536336363336330465353306053566453360460054500063446060346643400453633456446546354503346300344553655053503306444055363443403030604300064655506334560544330064304500443044350645463346060306564305053645046350533336636330660644345564364536063644433663304055544604505345553050664300303444544504030306344006654333400506636644330334035643434343650403030363503435603546463440550043403430450565505303434536654350405035004453066464566036403030463456364406563043554443345350063053566660564533336350334066344353343060333040443433504546433334030630044450544503643600503365633656300650303033036463444400546453433660605604345466536403330035505464066564030553033365340544530633043635653364030035430440366534344063556033036433354560635036440504640065344340360630645656404363635646435404000566355305364603035300453665355604454500366065300453056344643064506003040633033300355366455363044363454665305346560453446600355040336344366406666034430635645460030055644643436454555666535604534433533403656463455534635504300045363464434440365505040353066633435455505544440005566563444365006465334333654054465343003434400346465433440434444330364534565056636553350466366665444065035404305464055003066363646443506034046346345655363030563036546034303465663566303600330536343436333434335066330534556330543636533063664463044460404363630445655330334564353635300043333343453445004035566334003000404403305345440056456654646036363046453063443444334334364506344333565433446650504646560640503053635560330536553405563434335030303464065630335564353604646660446546334650646034645633405050533554434634053504534406566040353534050034445565535046605453535630533063453436463543544605066506040536044406563044440033040300305503465030335635304643503564033033543435664056553346400556663443446403556044360306553306600054344056563664053560300666335553436354636400330664340433306665444300563355536004040300460065555046560664664360660654646500503003630660436003664440605564505646630346355433505630004404465444303403544403353305434606435445550503645406353043335346060545366004536604054343043033336453304430466340466433534534506333403355666553334454634033053654043354440536650506533346333434535463464434035500535343436364653634550440500435666345606464665640553644040635346054344304443504646543556060034444434405605505646056505054364430650453306065566035536005634355634440355443436333404335360534644440440545463036040636333636444354445040543006605653655360446636564036306434633006446454004534466356330350345040344305300544054345636344534604554430666300406443045344343330335336056053464306403345334506006634436303045560650600460654463630034506506630605464035403660344436456553634040053405653564645635345400350400465565536430003533063334533606646403603630434654540343400453654430403000355660550305343654336630366533060345044400350004444543035603563050306033344665435655030343536346556005546633663660360430535500665336003655054033553303006404450450634554536533463300505503060036660060405034434464335060446450303043440050545544336343330060464646043364344636436445334043030353350433055033603455534663363030644354304633634530300666330045505005663635345635334344404364403545505543543604363456364430445430535344605443000656533404304643404640665353600035505645536635050336406044460044504306453444053003540444443605544643530663430364663560334463546064033535564663345340034036465533560355556644334504445306446435043330000534403446044565466064664063655546056533536530460566500553366043363563453456305046534303640053455366366565536434064604033444343434050564556306435335343633056430044454646560054446633606044350063444330044403453356403504340435330335464553065063505053666400500343035363406346543530544463644005400003640645653340666304453606660043403354334663054500044040500435660540656663334434336336363453544360300346304455653446334043306004353035654660606335543340534444563060645650564606434443463400404005455505436460500000063443330566564666363306643355000563405365655645656433406335063560663500653605654630534444345056343545305606664644306463400036046463034550033543340354543564563603564656054403630566030534404035343554053034360044364046365544355003463566306645363406335434546646603500540646504344500340435643630505536343436300063643530344436606330535535304636565404054634406555366353545345356400430305533664000345360535005653063443664606635660630534606340066306304566054633455365456503600054533434356446356440350630650453503444430334353043044603354035330305450633640405355666543060563433333660636354435463033530043635350430006000034363434564334333065006330564643463563440005464440305003046440443634503300000633634654433633665605665343540005454463634435034463446603640555543540533346640433530543355406440604656644404605530003603553300653533334050304644403600464446460453606646504345350053343646335604355344405540443303500303655540660333503345330533456454450036666053604334340533036430336553536600540563343006003330555604503650663344663353444343653450356546335533500654563304506665535636306644056464503445466556453004336536440406656533060344640545544403630555336443430440603353034634556566344464343330053634304340660403044333433450640535530554563303565553304360336430335065666366063533063460635034533643053064546066036360355364434403564403444643653546504430646436035564633466530303064346033500636400006563555364646663430053043636330443444434545340353456343360363635030063404036330605043460505456645343406653406403335555066043040504443034060506000333053004036035644500666303030065603630653035054003054460454534033546465335333665453554563066305405444433564564604563334034644435660463463063500334600045334336050344466654053533004446055543553633656640334605336503056633463644006603534363303406344503035354304433604636333045630035066356354333003344646665445540046440564540500305553344056334545404344365605544040065334635300654304564400406530555534306344666635536040454454444436343564066633305646650643634053335334450346434300360600534655040034065403404565303406534306645344344665640066054334000660353503050003646536350346350600553433050660034343635030336336564364336536654443540044030303306360554344433033543364443033605035353440336404453306045063060603036503644055540464463055550645444333065544403534354665356400640354460050465645363344350043656055435366634033045360365653504044350430656634436604333433530530363636043000634404606330665033005056345604340634436553363436656450433363543404503443454456645363344340505304503036030563360000633054430503045304005335335340306333430463054666443006544053550346363034035353044044550030334305466364063434460554606643556536343006544030365546660005356333450533003404553354035430466444034653606435543303606665035443353633353366300366536055446456430533333564440664036533353455503666365645400400534054630634306650646560044540456345503434634664636545360355430350633450635445333006563656464364643446356343355635663600560450644446543335604443546033464033400303666665060360360365044034645664400466434434466640536065430463504033443546044300363456045636464350666464460636040345546030360366400645365356404344450453635364363634040650305033063546363046606366355604300643604063466644305533353665303464333354663034354645433345356404404556040406434544454345464303645530336500050535433433630464536336550556636504340045344356466666665034350063335553505430456604346346334443665535660344444543440436406046000644564446436305305544355465343303333340446563453500534654465366650333634053335066336004304000060055640664003344000456644040504066063660543300630503034660036544435364064544600065434033533360406443565300034666534560043343563363656444456664540304650550045606004403606046455653433636355533330303005433456405563556303330034535666005666300366345365004433443540643066500004554664344055406333563345345063634064050634356006506443040446366333333065366665050603664335044366534444050063546304463335643530343343006033545443646653665404665454353435545403605353304536300003455300604463503306500036656530354004364545643440403360303066460443450044534633440603650306364346536405433464464433450000336334504465646643343430405433656534554434430460334463043030644430440004640630346064304443403544533436636563455064443633645633550035304403544045303630604436536403604363550443560300443660440655356450346533003335033366453455645443366460440534430645530340303456645434036366304346305034406003344456003006330665660646033446300463460364403450066534534633300330044646535665555654033505360334500405343404404635565605406503464556554030430005665306336060333036405644500643004460335653563304344034430304605434033303304405454663645064043033430045064660056634453300036344646636436335655565445354053640643034360665333366445660605464503305063353430443643563500553363343505035655654003556660353346400036554030300563036353664450033540463305646453460063600600454363656555364063303604665664363055004440336066640364504530634446446544335664646304033046344355433356633530363444333340353355005606665560435364053556360466006436460436034465430356350546445330534335345033334433063444300534443636600505004650556404454555663545454063033300363046430636056443505454530546005635343664004033543644033560035666556643630004345633604543354034435365045303633564433650664030006366653034653030034340446566604334663360344364453066003034330333530630003533634305350000004554046665404306553443504300635036560663465055645030345660436000445066605333033064453656303066364360666506306335444356354553060666340344505355550466603403455363646055545003033343433440350506633344666405465636056036400006046033563303345305365503466306334436633555006053034453443345035344340636030505463056036333045454006354630655450354430453346455030656535400434350533446034060346545343506646636355003300436054406046305450455444556456540030556344335060000464633630643444030634446404500603046365004036353554506650550504554500350604344634543445334400366366454450404545050655603366046533054654630433440564455444300653000606440605333644035433406430635\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}